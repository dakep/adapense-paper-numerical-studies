#' Generate Simulated Data For Scenarios 1 and 2
#'
#' The true coefficient vector is always a 0/1 vector of the form
#' `1, ..., 1, 0, ... 0` with `s` 1's and `p - s` 0's.
#' The predictors **X** are generated by a multivariate t-distribution with 4 degrees of freedom.
#'
#' The `settings` argument is a list with **all** of the the following elements:
#'  * `n` number of observations.
#'  * `p` number of predictors.
#'  * `s` number of active variables.
#'  * `cont` proportion of contamination.
#'  * `pve` percentage of variance explained.
#'  * `good_lev_p` number of predictors containing extreme values for good leverage points.
#'  * `good_lev_prop` proportion of observations with good leverage points.
#'  * `good_lev_pos` position of extreme values in good leverage points
#'                   (as a multiple of their original position).
#'  * `bad_lev_p` number of predictors containing extreme values for bad leverage points.
#'  * `bad_lev_pos` position of extreme values in bad leverage points
#'                   (as a multiple of their original position).
#'  * `k_vert` vertical outlier multiplier.
#'  * `resid_dist` distribution of the residuals. Must correspond to a valid distribution
#'                 family in R (e.g., `norm`, `cauchy`, `t(df = 5)`,
#'                 `stable(alpha = 1.33, beta = 0)`).
#'
#' @param settings data generation settings
#' @param test_n generate test data (i.e., without contamination).
#' @param test_seed the random seed to generate test data if `test_n` > 0.
#' @return a list with the predictor matrix in `x`, the response in `y` and
#'         the true regression coefficients in `beta`.
generate_data <- function (settings, test_n = 0L, test_seed) {
  if (!require('Matrix', quietly = TRUE)) {
    stop("`Matrix` package is required")
  }
  if (!requireNamespace('mvnfast', quietly = TRUE)) {
    stop("`mvnfast` package is required")
  }
  if (!require('stabledist', quietly = TRUE)) {
    stop("`stabledist` package is required")
  }
  if (!requireNamespace('pense', quietly = TRUE)) {
    stop("`pense` package is required")
  }
  if (!requireNamespace('stringr', quietly = TRUE)) {
    stop("`stringr` package is required")
  }

  if (!stringr::str_detect(settings$resid_dist, stringr::fixed('('))) {
    settings$resid_dist <- paste(settings$resid_dist, '()', sep = '')
  }

  generate_resid <- function (n, sd_err) {
    # If the residual distribution is not Normal, use `pense::tau_size` for the SD function
    sdfun <- if (stringr::str_starts(settings$resid_dist, stringr::fixed('norm'))) {
      stats::sd
    } else {
      pense::tau_size
    }
    call_obj <- str2lang(paste('r', settings$resid_dist, sep = ''))
    call_obj$n <- n
    r <- eval(call_obj)

    r * sd_err / sdfun(r)
  }

  ## Generate predictors
  generate_x <- if (identical(settings$pred_cor_type, 'grouped')) {
    function (n) {
      cor_between_groups <- 0.1
      sd_perturbation <- 0.2
      group_size <- 1L + floor(.5 * sqrt(settings$p))
      n_groups <- 1L + ceiling(settings$p / group_size)

      # Generate latent variables
      j <- seq_len(settings$p)
      j_latent_map <- 1L + floor((j - 1L) / group_size) + as.integer(j > settings$s)

      # Generate latent variables
      z <- mvnfast::rmvt(n, mu = numeric(n_groups), df = 4,
                         sigma = diag(1 - cor_between_groups, n_groups) + cor_between_groups)

      # Generate the predictor matrix
      z[ , j_latent_map] + rnorm(settings$p * n, sd = sd_perturbation)
    }
  } else if (identical(settings$pred_cor_type, 'ar1')) {
    function (n) {
      chol_decomp <- vapply(seq_len(settings$p), FUN.VALUE = numeric(settings$p),
                            FUN = function (i) {
                              d <- seq_len(settings$p) - i
                              (d >= 0) * settings$pred_cor_strength^d *
                                ((i == 1) + (i > 1) * sqrt(1 - settings$pred_cor_strength^2))
                            })
      mvnfast::rmvt(n, mu = numeric(settings$p), df = 4, sigma = t(chol_decomp), isChol = TRUE)
    }
  }

  ## Define true parameter vector
  beta <- sparseVector(rep.int(1, settings$s), seq_len(settings$s), settings$p)

  ## Generate predictors and model response
  x <- generate_x(settings$n)
  y_model <- as.numeric(x %*% beta)

  ## Determine contamination
  ncont <- as.integer(settings$n * settings$cont)
  cont_ind <- seq_len(ncont)

  # Generate bad leverage points
  cont_pred_bad <- integer(0L)
  if (isTRUE(settings$cont > 0) && isTRUE(settings$bad_lev_pos > 0)) {
    var_regfun <- var(y_model[-cont_ind])

    # Try not to contaminate truly active predictors, as this may artificially
    # increase sensitivity of affected estimates
    nr_cont_inactive <- with(settings, min(bad_lev_p, p - s))
    nr_cont_active <- with(settings, min(bad_lev_p_relevant, s))

    cont_pred_bad <- with(settings,
                          sort(c(sample.int(s, nr_cont_active),
                                 s + sample.int(p - s, nr_cont_inactive))))

    x_mahal <- mvnfast::maha(x[, cont_pred_bad], mu = numeric(length(cont_pred_bad)),
                             sigma = cov(x[, cont_pred_bad]))

    x[cont_ind, cont_pred_bad] <- x[cont_ind, cont_pred_bad] *
      sqrt(settings$bad_lev_pos * max(x_mahal) / x_mahal[cont_ind])
  } else {
    var_regfun <- var(y_model)
  }

  ## Determine error variance based on the given percentage of variance explained (PVE)
  sd_err <- sqrt((1 - settings$pve) * var_regfun / settings$pve)
  residuals <- generate_resid(settings$n, sd_err)

  ## Compute observed response
  y <- y_model + residuals

  ## Add vertical outliers, i.e., outliers in the response
  beta_cont <- beta
  if (isTRUE(settings$cont > 0) && is.numeric(settings$k_vert)) {
    beta_cont <- if (length(cont_pred_bad) > 0L) {
      sparseVector(rep.int(settings$k_vert, length(cont_pred_bad)), cont_pred_bad, settings$p)
    } else {
      sparseVector(rep.int(settings$k_vert, settings$s), seq_len(settings$s), settings$p)
    }

    # Set the PVE of the contamination to 0.909090 (i.e., SNR=10); sqrt(0.0909090 / 0.909090) = 1/sqrt(10)
    cont_y <- as.vector(x[cont_ind, ] %*% beta_cont)
    y[cont_ind] <- cont_y + rnorm(ncont, sd = sd(cont_y) / sqrt(10))
  }

  ## Introduce good leverage points in the last predictors
  if (isTRUE(settings$good_lev_prop > 0)) {
    cont_pred_good <- with(settings, rev(p - seq_len(good_lev_p)))
    x_clean_inactive <- if (length(cont_ind) > 0L) {
      x[-cont_ind, cont_pred_good, drop = FALSE]
    } else {
      x[ , cont_pred_good, drop = FALSE]
    }
    dist_rank <- rank(-mvnfast::maha(x_clean_inactive, numeric(length(cont_pred_good)),
                                     cov(x_clean_inactive)),
                      ties.method = 'first')
    good_lev_inds <- which(dist_rank <= with(settings, good_lev_prop * n))

    x_clean_inactive_ranks <- apply(abs(x_clean_inactive), 2, rank, ties.method = 'first')
    farthest_predictor <- apply(x_clean_inactive_ranks[good_lev_inds, , drop = FALSE], 1, which.max)

    if (length(cont_ind) > 0L) {
      good_lev_inds <- good_lev_inds + ncont
    }

    good_lev_cells <- (cont_pred_good[farthest_predictor] - 1L) * settings$n + good_lev_inds
    x[good_lev_cells] <- -settings$good_lev_pos * x[good_lev_cells]
  }

  test_x <- matrix(numeric(0), nrow = 0L, ncol = settings$p)
  test_y <- numeric(0)

  if (!missing(test_n) && isTRUE(test_n > 0)) {
    # Now generate the test data -- ensuring that we don't determine the random numbers
    # generated afterwards
    reset_seed <- sample.int(.Machine$integer.max, 1L)
    set.seed(test_seed)

    test_x <- generate_x(test_n)
    test_y_model <- as.vector(test_x %*% beta)
    test_y <- test_y_model + generate_resid(test_n, sd_err)

    # Make sure the RNG will produce different results for different initial
    # seeds!
    set.seed(reset_seed)
  }

  list(x = x, y = y,
       residuals = residuals,
       beta = beta,
       sd_err = sd_err,
       cont_pred_bad = cont_pred_bad,
       cont_pred_good = cont_pred_good,
       beta_cont = beta_cont,
       test_data = list(x = test_x, y = test_y),
       settings = settings)
}

